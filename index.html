<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mohammad Reza Taesiri</title>

  <meta name="author" content="Mohammad Reza Taesiri">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="./images/favicon.ico">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1WLD00VFNJ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-1WLD00VFNJ');
  </script>
  <script src="https://cdn.tailwindcss.com"></script>

  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            clifford: '#da373d',
          }
        }
      }
    }
  </script>

  <style>
    .strikethrough {
      position: relative;
      display: inline-block;
    }

    .strikethrough::before {
      content: "";
      position: absolute;
      height: 2px;
      width: 100%;
      background-image: linear-gradient(45deg, rgb(11, 0, 0), rgb(26, 5, 3));
      top: 50%;
      left: 0;
    }

    .oral-presentation {
      color: #FF0000;
      font-weight: bold;
    }
  </style>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                  <h1 class="text-3xl font-bold mb-4">Mohammad Reza Taesiri</h1>
                  </p>
                  <p>I recently graduated with a PhD from the <a href="https://www.ualberta.ca/index.html"
                      class="text-blue-600 hover:text-blue-800">University of
                      Alberta</a> in Edmonton, where I focused on the intersection of
                    computer vision, explainable machine
                    learning, and video games.</p>
                  <br>

                  <div class="flex flex-wrap justify-center mt-4">
                    <a href="mailto:mtaesiri@gmail.com" class="mr-2">Email</a>
                    <span class="mx-1">/</span>
                    <a href="data/Taesiri_CV.pdf" class="mx-2">CV</a>
                    <span class="mx-1">/</span>
                    <a href="https://scholar.google.com/citations?user=-egLZy8AAAAJ&hl=en" class="mx-2">Google
                      Scholar</a>
                    <span class="mx-1">/</span>
                    <a href="https://twitter.com/taesiri" class="mx-2">Twitter</a>
                    <span class="mx-1">/</span>
                    <a href="https://sketchfab.com/mtaesiri" class="mx-2">Sketchfab</a>
                    <span class="mx-1">/</span>
                    <a href="https://www.youtube.com/@mtaesiri" class="mx-2">Youtube</a>
                    <span class="mx-1">/</span>
                    <a href="https://huggingface.co/taesiri" class="mx-2">Hugging
                      Face</a>
                    <span class="mx-1">/</span>
                    <a href="https://wandb.ai/mtaesiri" class="mx-2">Wandb</a>
                    <span class="mx-1">/</span>
                    <a href="https://github.com/taesiri/" class="ml-2">Github</a>
                    <span class="mx-1">/</span>
                    <a href="https://taesiri.xyz/" class="ml-2">Hobby Projects</a>
                  </div>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/photo_2024-09-14 07.55.01.jpeg"><img style="width:100%;max-width:100%"
                      alt="profile photo" src="images/photo_2024-09-14 07.55.01.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <div class="mt-8">
            <h2 class="text-2xl font-bold mb-4">Research</h2>
            <p>
              I am broadly interested in computer vision and machine learning,
              with a focus on explainable machine
              learning. I am also interested in applying Foundation models to
              real-world problems, such as video
              game testing and quality control.
            </p>
          </div>
          <div class="mt-8">
            <h2 class="text-2xl font-bold mb-4">Recent Papers</h2>
          </div>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <br>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/hot.png" alt="HoT" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2503.02003">
                    <papertitle>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs
                  </a>
                  <br>

                  <a href="https://ngthanhtin.github.io/">Tin Nguyen</a>,
                  <a href="https://loganbolton.github.io/">Logan Bolton</a>,
                  <strong>Mohammad Reza Taesiri</strong>, and
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>

                  <br>
                  <em>ArXiv Preprint</em>, 2025
                  <p></p>

                  <a href="https://highlightedchainofthought.github.io/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/zero.png" alt="ZeroBench" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2502.09696">
                    <papertitle>ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models
                  </a>
                  <br>

                  <a href="#"></a>Jonathan Roberts</a>,
                  <strong>Mohammad Reza Taesiri</strong>, and
                  <a href="https://zerobench.github.io/">Others</a>

                  <br>
                  <em>ArXiv Preprint</em>, 2025
                  <p></p>

                  <a href=" https://zerobench.github.io/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/jonathan-roberts1/zerobench">
                    <papertitle>[Github]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/jonathan-roberts1/zerobench">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/vgb.png" alt="VideoGameBunny" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2407.15295">
                    <papertitle>VideoGameBunny: Towards vision assistants for video games
                  </a>
                  <br>

                  <strong>Mohammad Reza Taesiri</strong>, and
                  <a href="https://asgaard.ece.ualberta.ca/">Cor-Paul Bezemer</a>,

                  <br>
                  <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2025, <strong
                    class="oral-presentation"> (Oral
                    Presentation)</strong>
                  <p></p>

                  <a href="https://videogamebunny.github.io/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://huggingface.co/asgaardlab/VideoGameBunny-v1_0-4B">
                    <papertitle>[Model]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/asgaardlab/VideoGameBunny-Dataset">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/blinds.png" alt="VLMs Are Blind" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2407.06581">
                    <papertitle>Vision language models are blind <i class="fa-solid fa-glasses" aria-hidden="true"></i>

                  </a>
                  <br>

                  <a href="#"></a>Pooyan Rahmanzadehgervi</a>,
                  <a href="https://loganbolton.github.io/"></a>Logan Bolton</a>,
                  <strong>Mohammad Reza Taesiri</strong>, and
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>

                  <br>
                  <em>Asian Conference on Computer Vision (ACCV)</em>, 2024, <strong class="oral-presentation"> (Oral
                    Presentation)</strong>
                  <p></p>

                  <a href="https://vlmsareblind.github.io/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/anguyen8/vision-llms-are-blind">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/XAI/vlmsareblind">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/pcnn.png" alt="PCNN" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Nguyen_Allowing_Humans_to_Interactively_Guide_Machines_Where_to_Look_Does_CVPRW_2024_paper.pdf">
                    <papertitle>PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image
                      Classification Accuracy for AIs and Humans
                  </a>
                  <br>

                  <a href="https://giangnguyen2412.github.io/">Giang Nguyen</a>,
                  <a href="https://valeriechen.github.io/">Valerie Chen</a>,
                  <strong>Mohammad Reza Taesiri</strong>, and
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>

                  <br>
                  <em>Transactions on Machine Learning Research (TMLR)</em>, 2024
                  <p></p>
                  <a href="https://giangnguyen2412.github.io/PCNN/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/giangnguyen2412/PCNN-src-code-TMRL2024">
                    <papertitle>[Github]</papertitle>
                  </a>

                  <br>
                  <br>
                </td>
              </tr>




              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/batman.gif" alt="glitchbench" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2312.05291">
                    <papertitle>GlitchBench: Can large multimodal models detect
                      video game glitches?
                  </a>
                  <br>

                  <strong>Mohammad Reza Taesiri</strong>,
                  <a href="#"></a>Tianjun Feng</a>,
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>
                  <a href="https://asgaard.ece.ualberta.ca/">Cor-Paul Bezemer</a>,

                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                  <p></p>

                  <a href="https://glitchbench.github.io/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/GlitchBench/Benchmark">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/glitchbench/GlitchBench">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/chmcorrplusplus.png" alt="CHMCorr++" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Nguyen_Allowing_Humans_to_Interactively_Guide_Machines_Where_to_Look_Does_CVPRW_2024_paper.pdf">
                    <papertitle>Allowing humans to interactively guide machines where to look does not always improve a
                      human-AI team's classification
                      accuracy
                  </a>
                  <br>

                  <a href="https://giangnguyen2412.github.io/">Giang Nguyen</a>,
                  <strong>Mohammad Reza Taesiri</strong>,
                  <a href="https://sunniesuhyoung.github.io/">Sunnie S. Y. Kim</a>, and
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>

                  <br>
                  <em>CVPR Explainable AI for Computer Vision Workshop</em>, 2024
                  <p></p>

                  <a href="https://github.com/anguyen8/chm-corr-interactive">
                    <papertitle>[Github]</papertitle>
                  </a>

                  <br>
                  <br>
                </td>
              </tr>




              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/zoom.gif"
                    alt="Zoom  is what you need: An empirical study of the power of zoom and spatial biases in image classification"
                    width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2304.05538">
                    <papertitle>ImageNet-Hard: The Hardest Images Remaining
                      from a Study of the Power of Zoom and
                      Spatial Biases in Image Classification
                    </papertitle>
                    aka. <i>"Zoom is what you need: An empirical study of the
                      power of zoom
                      and
                      spatial biases in
                      image classification"</i>

                  </a>
                  <br>

                  <strong>Mohammad Reza Taesiri</strong>,
                  <a href="https://giangnguyen2412.github.io/">Giang Nguyen</a>,
                  <a href="https://habchisarra.github.io/">Sarra Habchi</a>,
                  <a href="https://asgaard.ece.ualberta.ca/">Cor-Paul Bezemer</a>,
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>

                  <br>
                  <em>Conference on Neural Information Processing Systems
                    (NeurIPS)</em>, 2023
                  <p></p>

                  <a href="https://taesiri.github.io/ZoomIsAllYouNeed/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/taesiri/ZoomIsAllYouNeed">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/taesiri/imagenet-hard">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/taesiri/imagenet-hard-4K">
                    <papertitle>[Dataset-4K]</papertitle>
                  </a>

                  <br>
                  <br>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/xai_corr.jpg" alt="Visual correspondence-based explanations" width="160"
                    height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2208.00780">
                    <papertitle>Visual correspondence-based explanations
                      improve AI robustness and human-AI team
                      accuracy</papertitle>
                  </a>
                  <br>
                  <strong>Mohammad Reza Taesiri
                    *</strong>,
                  <a href="https://giangnguyen2412.github.io/">Giang Nguyen</a>*,
                  <a href="https://anhnguyen.me/research/">Anh Nguyen</a>
                  (* denotes equal contribution)

                  <br>
                  <em>Conference on Neural Information Processing Systems
                    (NeurIPS)</em>, 2022
                  <p></p>

                  <a href="https://anhnguyen.me/project/correspondence-explanation/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="http://gpu3.cse.eng.auburn.edu:8000/">
                    <papertitle>[Live Demo]</papertitle>
                  </a>
                  <a href="https://github.com/anguyen8/visual-correspondence-XAI">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://www.youtube.com/watch?v=lnHFOqahHSU&t=13s">
                    <papertitle>[Video]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/clip_gamephysics.gif" alt="CLIP Meets GamePhysics" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2203.11096">
                    <papertitle>CLIP meets GamePhysics: Towards bug
                      identification in gameplay videos using zero-shot
                      transfer learning</papertitle>
                  </a>
                  <br>
                  <strong>Mohammad Reza Taesiri</strong>, Finlay Macklon,
                  Cor-Paul Bezemer
                  <br>
                  <em>The Mining Software Repositories (MSR) conference</em>,
                  2022
                  <br>
                  <br>
                  <a href="https://asgaardlab.github.io/CLIPxGamePhysics/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/asgaardlab/CLIPxGamePhysics">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://huggingface.co/spaces/taesiri/CLIPxGamePhysics">
                    <papertitle>[Live Demo]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/taesiri/GamePhysics">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                  <br>
                  <br>
                </td>
              </tr>

              <tr onmouseout="mira_stop()" onmouseover="mira_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src="images/llm_bugs.gif" width="160">
                  </div>
                  <script type="text/javascript">
                    function mira_start() {
                      document.getElementById('mira_image').style.opacity = "1";
                    }

                    function mira_stop() {
                      document.getElementById('mira_image').style.opacity = "0";
                    }
                    mira_stop()
                  </script>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2210.02506">
                    <papertitle>Large Language Models are Pretty Good
                      Zero-Shot Video Game Bug Detectors</papertitle>
                  </a>
                  <br>
                  <strong>Mohammad Reza Taesiri</strong>, Finlay Macklon, Yihe
                  Wang, Hengshuo Shen, Cor-Paul Bezemer
                  <br>
                  <em>ArXiv Preprint</em>, 2022
                  <br>
                  <br>
                  <a href="https://asgaardlab.github.io/LLMxBugs/">
                    <papertitle>[Website]</papertitle>
                  </a>
                  <a href="https://github.com/asgaardlab/LLMxBugs">
                    <papertitle>[Code]</papertitle>
                  </a>
                  <a href="https://huggingface.co/datasets/taesiri/GameBugDescription">
                    <papertitle>[Dataset]</papertitle>
                  </a>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p align="right">
                    <font size="2">
                      Website design by <a href="https://people.eecs.berkeley.edu/~barron/">Jon
                        Barron</a>.
                    </font>
                  </p>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body </html>